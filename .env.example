# API Authentication
# IMPORTANT: If you change this API key, you MUST rebuild the frontend container:
#   docker compose build webui-frontend && docker compose up -d webui-frontend
# The API key is baked into the frontend at build time via Vite.
API_KEY=sk-local-your-secret-key-here

# Model Configuration
PYTHON_MODEL=TheBloke/deepseek-coder-33B-instruct-AWQ
GENERAL_MODEL=TheBloke/Mistral-7B-v0.1-AWQ
GPT_OSS_120B_MODEL=openai/gpt-oss-120b
GPT_OSS_20B_MODEL=openai/gpt-oss-20b

# Service Ports
ROUTER_PORT=8080
WEBUI_PORT=3000
CODER_BACKEND_PORT=8000
GENERAL_BACKEND_PORT=8001
GPT_OSS_120B_PORT=8002
GPT_OSS_20B_PORT=8003
METRICS_PORT=9090

# Performance Tuning (GPU Memory Utilization)
# These values represent the fraction of total GPU memory (95.6GB) reserved for each model
# Includes: model weights + KV cache + activations + overhead
CODER_GPU_MEMORY=0.45      # DeepSeek Coder 33B: 43GB (model: 17GB + KV cache: 26GB)
GENERAL_GPU_MEMORY=0.10    # Mistral 7B: 9.5GB (model: 4GB + KV cache: 5.5GB)
GPT_OSS_120B_GPU_MEMORY=0.85  # GPT-OSS 120B: 81GB (model: 122GB on disk, quantized in memory)
GPT_OSS_20B_GPU_MEMORY=0.20   # GPT-OSS 20B: 19GB (model: 26GB on disk, quantized in memory)
CODER_MAX_SEQ=64
GENERAL_MAX_SEQ=128
GPT_OSS_120B_MAX_SEQ=256
GPT_OSS_20B_MAX_SEQ=512

# vLLM Configuration
CODER_MAX_MODEL_LEN=4096
GENERAL_MAX_MODEL_LEN=4096
GPT_OSS_120B_MAX_MODEL_LEN=8192
GPT_OSS_20B_MAX_MODEL_LEN=8192
CODER_MAX_BATCHED_TOKENS=8192
GENERAL_MAX_BATCHED_TOKENS=8192
GPT_OSS_120B_MAX_BATCHED_TOKENS=16384
GPT_OSS_20B_MAX_BATCHED_TOKENS=16384

# Logging
LOG_LEVEL=INFO
LOG_RETENTION_DAYS=7
LOG_MAX_SIZE_GB=10

# Docker Configuration
COMPOSE_PROJECT_NAME=local-llm-service
